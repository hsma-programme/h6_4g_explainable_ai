# HSMA Session 4G - Explainable AI

Students should be able to:

- Give examples to demonstrate why explainable AI is important
- Explain the difference between model-agnostic and model-specific solutions
- Explain the difference between global and local explainability
- Explain the benefits of model-agnostic explainable AI techniques
- Name some of the techniques available for model-agnostic explainable AI
- Explain how to interpret feature importance coefficients from logistic regression models
- Explain the theory of mean decrease in impurity (MDI) for feature importance in tree based model
- Explain the theory of permutation feature importance (PFI) for model-agnostic calculation of feature importance
- Calculate MDI and PFI and visualise the outputs
- Explain what partial dependence plots (PDPs) are
- Create partial dependence plots (PDPs)
- Explain what Individual Conditional Expectation (ICE) plots are
- Create Individual Conditional Expectation (ICE) plots
- Explain the history and theory behind SHAP
- Interpret SHAP plots
- Use the shap python library to create plots for both global and local explainability
- List some of the criticisms of different explainability techniques

## Credits

Original versions of shap teaching notebooks (titanic and mpg) and exercise 1 (iris dataset) created by [Elliot Coyne](https://github.com/ElliottHSMA) for [HSMA 5 masterclass](https://github.com/hsma-programme/h5_masterclass_shap/tree/main)
